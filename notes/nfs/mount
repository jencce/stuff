fs/nfs/inode.c
	module(init_nfs_fs) --> register_nfs_fs --> regster_fetm(&nfs4_fs_type)
						    regster_fletm(&nfs_fs_type)

struct file_system_type nfs_fs_type / nfs4_fs_type = {
        .owner          = THIS_MODULE,
        .name           = "nfs", / "nfs4",
        .mount          = nfs_fs_mount,       <-------------------------- POINT
        .kill_sb        = nfs_kill_super,
        .fs_flags       = FS_RENAME_DOES_D_MOVE|FS_BINARY_MOUNTDATA,
};


fs/namespace.c
	mount syscall --> do_mount --> do_new_mount --> XXX

	---------------------------------------------------------------
	mount -t nfs4  localhost:/root /nfsmnt
			||
			\/
	SYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name,
                char __user *, type, unsigned long, flags, void __user *, data)
			||
			\/
	jprobe: dev_name = localhost:/root, dir_name = /nfsmnt, type = nfs4,
	flags = 0x0, data = addr=::1,clientaddr=::1
	---------------------------------------------------------------

	IN do_mount, done mountpoint dir_name lookup, flags transformation,
	separate per-mountpoint flags to mnt_flags, clear useless flags:

	flags &= ~(MS_NOSUID | MS_NOEXEC | MS_NODEV | MS_ACTIVE | MS_BORN |
                   MS_NOATIME | MS_NODIRATIME | MS_RELATIME| MS_KERNMOUNT |
                   MS_STRICTATIME);	

	and CALL:
	do_new_mount(&path, type_page, flags, mnt_flags, dev_name, data_page);

	IN do_new_mount, call get_fs_type to get nfs type registered in
	filesystems list,
	then CALL:
	vfs_kern_mount(struct file_system_type *type, int flags,
			const char *name, void *data)
	and call do_add_mount if success.

	XXX --> vfs_kern_mount --> mount_fs --> type->mount ==> nfs_fs_mount

	IN mount_fs, CALL:
		root = type->mount(type, flags, name, data);
	so comes nfs_fs_mount.


fs/nfs/super.c
	nfs_fs_mount --> nfs4_try_mount --> nfs_do_root_mount

	struct file_system_type nfs4_fs_type = {
        	.owner          = THIS_MODULE,
        	.name           = "nfs4",
        	.mount          = nfs_fs_mount,
        	.kill_sb        = nfs_kill_super,
        	.fs_flags       = FS_RENAME_DOES_D_MOVE|FS_BINARY_MOUNTDATA,
	};


	IN nfs_fs_mount,
	init struct nfs_mount_info mount_info, includes options and fhandle etc,
	then  validate all of them; 
	init struct nfs_subversion *nfs_mod, acorrding to fs_type,
	then CALL:
		mntroot = nfs_mod->rpc_ops->try_mount(flags, dev_name,
							&mount_info, nfs_mod);


	So we come to one particular nfs_subversion, eg nfs_v4, roc_ops struct:

fs/nfs/nfs4super.c
	IN nfs4_try_mount,
	replace export_path with "/", do_root_mount with remote_fs_type first,
	then replace back real export_path, call nfs_follow_remote_path:

	-------------------------------------------------------
	export_path = data->nfs_server.export_path;
        data->nfs_server.export_path = "/";
        root_mnt = nfs_do_root_mount(&nfs4_remote_fs_type, flags, mount_info,
					//   ^^^ NOTICE HERE
                        data->nfs_server.hostname);
        data->nfs_server.export_path = export_path;
        res = nfs_follow_remote_path(root_mnt, export_path);
	-------------------------------------------------------
	

	IN nfs_do_root_mount, just handle hostname, then CALL:
		vfs_kern_mount(fs_type, flags, root_devname, data);
	as vfs_kern_mount called before, with nfs4_remote_fs_type:

		static struct file_system_type nfs4_remote_fs_type = {
	        	.owner          = THIS_MODULE,
	        	.name           = "nfs4",
		        .mount          = nfs4_remote_mount,
        		.kill_sb        = nfs_kill_super,
		        .fs_flags       = FS_RENAME_DOES_D_MOVE|FS_BINARY_MOUNTDATA,
		};

	vfs_kern_mount --> mount_fs --> nfs4_remote_mount

	IN nfs4_remote_mount,
	first call server = nfs4_create_server(mount_info, &nfs_v4);
	then nfs_fs_mount_common(server, flags, dev_name, mount_info, &nfs_v4);

	nfs4_remote_mount --> nfs4_create_server
			  |
			  --> nfs_fs_mount_common

fs/nfs/nfs4client.c
	nfs4_create_server --> nfs4_init_server
			   |
			   --> nfs4_server_common_setup

	IN nfs4_create_server,
	alloc struct nfs_server and init some fields,
	>>>call nfs4_init_server to setup client stuff,
		nfs4_init_server(server, mount_info->parsed);
	>>>call nfs4_server_common_setup to setup.
		nfs4_server_common_setup(server, mnt_info->mtfh, auth_probe)



	nfs4_init_server --> nfs4_set_client
			 |
			  --> nfs_init_server_rpcclient

	IN nfs4_init_server:
	continue init struct nfs_server with parsed data,
	>>>call nfs4_set_client with strcut and some mount info,
	>>>call nfs_init_server_rpcclient.

	IN nfs4_set_client:
	>>>call nfs_get_client:
	find a nfs_client matches parsed data if it exists, OR alloc and init
	a nfs_client with nfsv4 rpc_ops funcs,

	nfs4_set_client --> nfs_get_client --> nfs4_init_client
						|
						v
			rpc_create <-- nfs_create_rpc_client

	IN nfs4_init_client:
	>>>call nfs_create_rpc_client with RPC_AUTH_GSS_KRB5I, if fail then with
	RPC_AUTH_UNIX in the flavor parameter,
	>>>call nfs_idmap_new and other funcs, TBD

fs/nfs/client.c
	IN nfs_create_rpc_client,
	init struct rpc_create_args, with flavor argument, and init
	rpc_program to nfs_program:

	const struct rpc_program nfs_program = {
	        .name                   = "nfs",
        	.number                 = NFS_PROGRAM,
	        .nrvers                 = ARRAY_SIZE(nfs_version),
        	.version                = nfs_version,
	        .stats                  = &nfs_rpcstat,
        	.pipe_dir_name          = NFS_PIPE_DIRNAME,
	};

	then call rpc_create.

net/sunrpc/clnt.c
	rpc_create -> xprt_create_transport -> xprt_class->setup(xs_setup_tcp)
		   |
		   --> rpc_create_xprt

	IN rpc_create,
	create a RPC transport with xprt_create_transport .. 
	then call rpc_create_xprt create an client with transport and return.
	
	IN xprt_create_transport,
	find the right xprt_class in xprt_list for this RPC, classes are ident
	with below categories:
	
	static struct xprt_class xs_tcp_transport = {
	        .list           = LIST_HEAD_INIT(xs_tcp_transport.list),
        	.name           = "tcp",
	        .owner          = THIS_MODULE,
        	.ident          = XPRT_TRANSPORT_TCP,
	        .setup          = xs_setup_tcp,
	};

	enum xprt_transports {
        	XPRT_TRANSPORT_UDP      = IPPROTO_UDP,
        	XPRT_TRANSPORT_TCP      = IPPROTO_TCP,
	        XPRT_TRANSPORT_BC_TCP   = IPPROTO_TCP | XPRT_TRANSPORT_BC,
        	XPRT_TRANSPORT_RDMA     = 256,
       		XPRT_TRANSPORT_LOCAL    = 257,
	};

	then call the clas->setup function.

net/sunrpc/xprtsock.c
	xs_setup_tcp --> xs_setup_xprt

	IN xs_setup_tcp:
	>>>call xs_setup_xprt to setup a struct rpc_xprt, including alloc and init
	list and wait queues, copy addrs and init a struct sock_xprt to hold
	info about tcp/addrs, sock_xprt is a member of rpc_xprt, init 
	rpc_xprt->ops to xs_tcp_ops:

	static struct rpc_xprt_ops xs_tcp_ops = {
        .reserve_xprt           = xprt_reserve_xprt,
	        .release_xprt           = xs_tcp_release_xprt,
	        .alloc_slot             = xprt_lock_and_alloc_slot,
	        .rpcbind                = rpcb_getport_async,
	        .set_port               = xs_set_port,
	        .connect                = xs_connect,
	        .buf_alloc              = rpc_malloc,
	        .buf_free               = rpc_free,
	        .send_request           = xs_tcp_send_request,
	        .set_retrans_timeout    = xprt_set_retrans_timeout_def,
	        .close                  = xs_tcp_close,
	        .destroy                = xs_destroy,
	        .print_stats            = xs_tcp_print_stats,
	};

	after setup, comes back to xprt_create_transport, done some init
	work about name and queue and timer,
	then comes back to rpc_create.

net/sunrpc/clnt.c
	rpc_create -> xprt_create_transport -> xprt_class->setup(xs_setup_tcp)
		   |
		   --> rpc_create_xprt --> rpc_new_clnt -> rpc_client_register

	after xprt_create_transport, there is resvport handle,
	then comes to rpc_create_xprt.

	IN rpc_create_xprt:
	>>>call rpc_new_clnt to setup a new struct rpc_clnt,
	if RPC_CLNT_CREATE_NOPING not set, call rpc_ping(clnt),
	then, init and set some flags and then return clnt.
	
	IN rpc_new_clnt:
	>>>call rpciod_up, bring up rpciod for each clnt?, alloc struct rpc_clnt,
	alloc clnt id for idr rpc_clids, init rpc_clnt fields, program, 
	lists, rtt, ndname,and rpc_xport to clnt->clnt_xprt with rcu protect?,
	Finally call rpc_client_register.

	IN rpc_client_register:
	setup pipefs_sb and dir, according to rpc net namespace,
	register clnt to ns->sunrpc_net->all_clients list,
	>>>call rpcauth_create to create rpc_auth, request diff module according
	to diff flavor, first GSS, then UNIX, increase count etc, then assign
	to clnt->cl_auth. Related to gss-krb* module etc.
	
	rpc_create -> xprt_create_transport -> xprt_class->setup(xs_setup_tcp)
		   |
		   --> rpc_create_xprt --> rpc_new_clnt -> rpc_client_register
				      |
				      ---> rpc_ping(clnt) -> rpc_call_sync

	NOW, back to rpc_create_xprt:
	calling rpc_ping,

	IN rpc_ping,
	init rpc_message msg, proc, cred, etc, rpc_proc is null call,
	>>>call rpc_call_sync to perform a RPC call, according to tshark, mount will
	do this as first two rpc calls.
	
	##So, mount -> init_a_server -> init_a_client -> rpc_ping_call

	rpc_call_sync -> rpc_run_task -> rpc_execute -> __rpc_execute[RPC scheduler]

	IN rpc_call_sync,
	>>>call rpc_run_task with a struct rpc_task_setup:
		struct rpc_task_setup task_setup_data = {
	                .rpc_client = clnt,
	                .rpc_message = msg,
	                .callback_ops = &rpc_default_ops,
	                .flags = flags,
	        };
	
	then rpc_call_start, to init task->tk_action = call_start,
	so rpc_execute can execute the rpc call flow:

		start -> reserve -> bind/refresh_cred -> encode_xdr -
								    |
								    v
               clean/handle_timeout <- transmit_request <- rpc_bind/connect
			|
			v
                 decode_reply
 
	NOW, back to rpc_create_xprt:
	after calling rpc_ping, do some check and return the clnt.
	come back to rpc_create,  
	come back to nfs_create_rpc_client, clp->cl_rpcclient = clnt,
	come back to nfs4_init_client,
	
	nfs4_set_client --> nfs_get_client --> nfs4_init_client
						|
						v
			rpc_create <-- nfs_create_rpc_client

	IN nfs4_init_client:
	>>>call nfs_create_rpc_client with RPC_AUTH_GSS_KRB5I, if fail then with
	RPC_AUTH_UNIX in the flavor parameter, inited a nfs_client,
	>>>call nfs_idmap_new, inited a struct idmap for clnt:
		struct idmap { 
 	        	struct rpc_pipe_dir_object idmap_pdo;
	        	struct rpc_pipe         *idmap_pipe;
	        	struct idmap_legacy_upcalldata *idmap_upcall_data;
	        	struct mutex            idmap_mutex;
		};

	>>>call nfs4_init_client_minor_version, clnt->cl_mvops->init_client,
	init other minor version related stuff:
		nfs40_init_client:  init slot table
		nfs41_init_client:  init session
	>>>call nfs4_init_callback, init nfs4 callback service, first setup
	callback channels, init several at least 1 rpc_rqst, add to list
	clnt->cl_rpcclient->cl_xprt xprt->bc_pa_list, second bring up the
	callback service thread which is a RPC service daemon receiving and
	processing incoming RPC messages.

	mark clnt ready by NFS_CS_READY,

	>>>call nfs4_discover_server_trunking ->
		clp->cl_mvops->reboot_recovery_ops->detect_trunking, >
	as nfs40_discover_server_trunking, conduct SETCLIENTID rpc call, 
	negotiate client id with server, then call nfs40_walk_client_list,
	find the client like out clnt most in nfs_net->nfs_client_list,
	use the found new one conduct RPC call SETCLIENTID_CONFIRM,
	swap their callback id and free clnt, return the found new one.
	WHY?

	SO form nfs4_init_client RETURN to nfs_get_client,
				 RETURN to nfs_set_client,

	INI nfs_set_client, setup the clnt we got to server member:
		server->nfs_client = clp, 

	then RETURN back to nfs4_init_server:

	nfs4_init_server --> nfs4_set_client
			 |
			  --> nfs_init_server_rpcclient

	IN nfs4_init_server:
	continue init struct nfs_server with parsed data,
	>>>call nfs4_set_client with strcut and some mount info, to setup
		a clnt in server;
	then init server ac prorperties and port, then
	>>>call nfs_init_server_rpcclient, clone a rpc_clnt from nfs_clnt,
		to server->client, from code comment:
		nfs_server->nfs_clent  :  shared client and NFS4 state
		nfs_server->client     :  RPC client handle

	then RETURN to nfs4_create_server,
	


Then vfs_kern_mount call registered function nfs4_get_sb do most of the work.
nfs4_get_sb --> nfs4_try_mount --> nfs_do_root_mount
Then  goes to nfs4_remote_get_sb --> nfs4_create_server --> nfs4_init_server --> nfs4_set_client 


Back to nfs4_try_mount, after nfs_do_root_mount, calls nfs_follow_remote_path --> vfs_path_lookup --> path_walk --> .. -> do_lookup --> follow_managed --> follow_automount --> dentry->d_op->d_automount
