perf notes
https://perf.wiki.kernel.org/index.php/Tutorial

获得编译安装：
在kernel的git目录下：make perf-targz-src-pkg，得到压缩包；
解压；
cd perf-xxx/tools/perf
make
./perf就可以用了。

perf是用来分析Linux的性能的探测工具。是Linux的一部分。

使用：
像git，perf有很多子命令，来收集和分析 性能和跟踪 数据。

list
列出可以记录的事件event，事件的来源有多个。

有些是纯kernel的计数器，比如cs，mf等，这样的称为software events。


有些事件和cpu模型相关，不能直接写成symbolic form，就要使用模型相关
的编号，称为raw hardware event。
这些编号和其对应的事件，需要去cpu的软件开发手册中查找。

和cpu相关的事件是cpu或者cpu的Performance Monitoring Unit提供的事件。
这些称为PMU hardware event。perf还提供了这些hw事件的一个子集，是一些平常
的hardware event，称为hw event/hw cached events。这个子集对应的事件
要真的被cpu支持才能生效。

还有一部分是称为tracepoint events的事件，就是kernel ftrace接口实现的
事件，只在2.6.3x中被支持。

symbolic event types。
事件被指定时可以带一个或者多个修饰符，被冒号分隔：
u 用户空间记录
k 内核空间记录
h hypervisor记录
G kvm guest记录
H kvm host记录
p 精确度
p可以出现多次，指定指令的地址应该有多精确，0-4次越来越精确。由于cpu
的限制顶多2。
如 instructions:p
所有的事件都有sub-events，有些情况下可以监测所有的子事件；事件也能有修饰符
和过滤器，来控制时间如何被计数。

stat
Counting with perf stat
对于任何的事件，perf在进程执行期间对事件持续计数。在计数模式下，事件被简单得
累计和输出。
不指定事件的情况下，perf stat ls。stat收集一些普通的事件，如cs，cycles等。
可以指定一个或多个被收集的事件，通过指定事件的symbolic name，以及可能跟随
在后面的unit mask和modifiers。这些都是大小写敏感。
	perf stat -e cycles:u ls
u就是modifiers，见上面的表。

指定多个事件：
perf stat -e cycles,cs ls
perf stat -e cycles -e cs ls
没有个数的限制，如果事件数超过了计数器个数，内核会复用；可以同时测量多个不同来源
的事件。只是每个事件提供一个文件描述符，或者每个线程（per-thread mode）或者
每个cpu(system-wide)一个文件，那么可能超过kernel对每个线程打开文件数目的上
限，这样会报错。

当事件的数目超过计数器的个数时，内核使用复用，切换的频率通常是100/1000，给每个
事件以机会被计数。复用只对PMU事件有效。复用的时候，事件不是一直被监测，而是以实
际监测的时间和需要监测的时间估算一个值。在运行的最后，perf估算了一个scale值:
final_cnt=raw_cnt*time_enabled/time_run。
这是个估计值，在负载变化时可能会不准确。
多个事件是以round-robin方式管理的。cpu的计数器也不全部一样，有的多有的少，有的
计数器能计数所有的事件，有的只能特定的事件。避免scale的话，可以减少事件的个数。
下面的例子显示了scaling：
perf stat -B -e cycles,cycles ls //一般没有scaling，两个cnt一样
perf stat -B -e cycles,cycles,cycles,cycles ls //如果显示的count不再
一样，说明有了scaling。

可以使命令运行重复运行来测试：
perf stat -B -r 5 -e cycles ls，执行五次ls

控制事件检测的环境
四种模式：per-thread, per-process, per-cpu, system-wide。
意思都很明显。

涉及进程和线程的监测，perf默认子进程继承计数器，-i选项不继承。
-a表示所有cpu,system-wide模式，-C选项可以指定cpu的范围。。

perf默认是per-thread模式，就是只对指定的命令执行的进程和所有子进程来收集计数信
息，如果指定-a选项，表示进入system-wide模式，此时不光收集指定命令的信息，还收
集所有cpu上运行的所有进程的计数信息，这时收集的时限就是指定命令运行的时间。

-p和-t选项可以指定进程和线程来收集信息，时限是指定命令的执行时间，如果时间内指定
的进程或者线程没有运行则不会有信息被收集到。不推荐跟踪内核线程，它们一般运行在固定
cpu上，可以使用system-wide模式来收集cpu的信息。

输出选项
-B启用locale的数字分隔。
-x , 输出方便程序处理的数据格式，以指定的，为分隔符。


record 
Sampling with perf record
运行一个命令，记录下其性能数据到perf.data。稍后使用perf report,annotate解析
这个数据文件，可以在另外一台机器上。

-e和--event选项可以指定需要记录的事件，包含三种类型的事件：
1是symbolic event，也就是perf list可以列出来所有的events；
2是raw hardware events；
3是hardware breakpoint，比如\mem:addr[:access]，如果想记录地址
0x100的读写操作，那么对应的event就是\mem:0x100:rw

perf_events是基于 基于事件的采样，period是以事件出现的次数来表示的，而不是时间
tick数。当一个采样计数器溢出的时候，就记录一个采样。

虽然硬件不支持64位的计数器，kernel模拟了64位计数，但是这受限与硬件真正的位数，
需要在硬件溢出的时候产生sampling period。

当计数器溢出的时候记录信息，就是一个采样，信息关于程序的执行，种类可以由工具指定。
但是最平常的是指令指针。
基于中断的采样的时候，采样时候存储的指针指向程序被PMU中断中断的地方，而不是计数器
溢出的地方。这样两个采样点之间，执行的指令数不是确定的，慎重。

默认事件就是cycles，周期，通过内核映射到intel/amd的cpu中的硬件事件。当cpu空
闲idle时，不会计数。

两种方式指定采样周期sampling period：
a事件出现的次数
b一秒采样的平均频率samples/sec, frequence
默认使用的是平均频率，1000s/s，这种情况下内核根据频率动态调整采样周期来满足这个
平均频率，频率可以使用-F选项指定。另外一种模式下period由用户-c选项指定，不会变。

perf record默认是per-thread继承模式，事件是cycles.

-a 从全系统所有的cpu中收集信息
-l scale counter values?
-p/-t/-u 指定pid/thread id/user id收集
-c 采样的event period个数
-F 指定采样的频率

report
Sample analysis with perf report
读取record命令产生的perf.data数据文件，显示其中的profile信息。
输出的各项意思在wiki上查：
perf report

# Events: 1K cycles
#
# Overhead          Command                   Shared Object
                    Symbol
# ........  ...............  ..............................
.....................................
#
   28.15%      firefox-bin  libxul.so                       [.] 0xd10b45
    4.45%          swapper  [kernel.kallsyms]               [k] mwait_idle_with_hints
    4.26%          swapper  [kernel.kallsyms]               [k] read_hpet
    2.13%      firefox-bin  firefox-bin                     [.] 0x1e3d
    1.40%  unity-panel-ser  libglib-2.0.so.0.2800.6         [.] 0x886f1
    [...]
The column 'Overhead' indicates the percentage of the overall samples collected in the corresponding function. 
第一列overhead表示事件样本在各个（函数/进程）中采集到的比例；
The second column reports the process from which the samples were collected.
第二列表示从哪个进程采样到的事件； 
(In per-thread/per-process mode, this is always the name of the monitored command. But in cpu-wide mode, the command can vary. )
The third column shows the name of the ELF image where the samples came from.
第三列表示事件进程来源的ELF文件，动态库或者kernel； 
If a program is dynamically linked, then this may show the name of a shared library. 
When the samples come from the kernel, then the pseudo ELF image name [kernel.kallsyms] is used. 
The fourth column indicates the privilege level at which the sample was taken, 
i.e. when the program was running when it was interrupted:
第四列表示采样事件来源的优先级，如下所述：
[.] : user level
[k]: kernel level
[g]: guest kernel level (virtualization)
[u]: guest os user space
[H]: hypervisor
The final column shows the symbol name.
最后一列是符号名称。

默认按照样本的多少排序的，可以通过选项指定排序key。--sort=cpu/pid/dso
可以查看产生时间最多的cpu/pid/dso等。

-k选项指定未压缩的vmlinux文件的路径，需要内核带debug symbols编译。
report出来最后一列只有地址没有符号的，是elf被strip了，没有符号表。

annotate
Source level analysis with perf annotate
使用这个命令可以钻到指令级别，所有的函数会被反汇编，每条指令会有其相关的事件比例。
一眼能看出哪些指令执行的多一些。
如果程序使用-ggdb选项编译，那么可以产生源代码级别的信息。
如果要看内核的信息，同样需要-k指定未压缩的带debug信息的vmlinux文件。
例子：
         :        void init_ob()                                               ▒
         :        {                                                            ▒
    0.00 :         80495c8:       push   %ebp                                  ▒
    0.00 :         80495c9:       mov    %esp,%ebp                             ▒
    0.00 :         80495cb:       push   %edi                                  ▒
    0.00 :         80495cc:       push   %ebx                                  ▒
    0.00 :         80495cd:       sub    $0x10,%esp                            ▒
         :                int i = 0;                                           ▒
    0.00 :         80495d0:       movl   $0x0,-0xc(%ebp)                       ▒
         :                obindex = 0;                                         ▒
    0.00 :         80495d7:       movl   $0x0,0x804d0cc                        ▒
         :                while (i < BUFSIZ)                                   ▒
    0.00 :         80495e1:       jmp    8049649 <init_ob+0x81>                ▒
         :                        memset(out_buff[i++], '\0', BUFSIZ);         ▒
    0.00 :         80495e3:       mov    -0xc(%ebp),%eax   


top
Live analysis with perf top
像top一样，“实时”显示采样的函数。默认事件是cycles，默认模式是system-wide。也
可以使用-C选项指定cpu。
在界面中进一步操作，看到更详细的信息，有符号表可以跟踪到函数。



Troubleshooting and Tips

open file limits
perf给每个事件每个线程每个cpu打开一个文件来记录，
perf record ./100threds
如果是在16个cpu的系统中，那么会创建1*100*16=1600个文件
可以使用ulimit 提高打开文件数的上限，或者控制计数事件和cpu的数量。

builid-cache
perf record会在perf.data中记录和记录过程中相关的elf文件的build-id。
build-id由链接器ld的-Wl --build-id选项产生，可选算法，同一个程序在同一个系统
上可以相同可以不同。build-id用来定位debug信息，找到其相关的文件。
record在最后会去更新build-id cache，由包含采样的build-id和elf文件组成。
cache默认在￥home/.debug。
有时候不让record更新这个cache更好，使用-N选项。

perf script
字符界面显示report

perf inject
给现有的event stream加入额外的信息。

perf sched
Tool to trace/measure scheduler properties (latencies)
perf sched record <cmd> 记录schedule事件
perf sched latency 报告cmd的sched延迟和其他sched信息
perf sched replay 模拟record的时候的负载，回放，可以多次
perf sched map  打印一个文字上下文切换的概述，一个一个cpu，有事件的标记*，没
事件的标记.。



寻找瓶颈的例子：
1、对比自己程序和系统程序的stat:
系统程序：
zx@M2420@coreutils$ perf stat ls
abc	 ls    Makefile   perf.data.old  README.md  tmp
LICENSE  ls.c  perf.data  perfnotes	 test.sh    TODO

 Performance counter stats for 'ls':

          2.595600 task-clock                #    0.772 CPUs utilized          
                 8 context-switches          #    0.003 M/sec                  
                 0 CPU-migrations            #    0.000 M/sec                  
               253 page-faults               #    0.097 M/sec                  
         1,897,482 cycles                    #    0.731 GHz                     [50.75%]
         1,215,870 stalled-cycles-frontend   #   64.08% frontend cycles idle   
           954,658 stalled-cycles-backend    #   50.31% backend  cycles idle   
         1,425,758 instructions              #    0.75  insns per cycle        
                                             #    0.85  stalled cycles per insn
           261,917 branches                  #  100.908 M/sec                  
            14,565 branch-misses             #    5.56% of all branches         [53.51%]

       0.003362527 seconds time elapsed

自己程序：
zx@M2420@coreutils$ perf stat ./ls
abc      ls    Makefile   perf.data.old  README.md  tmp 
LICENSE  ls.c  perf.data  perfnotes      test.sh    TODO

 Performance counter stats for './ls':

         63.680247 task-clock                #    0.987 CPUs utilized          
                 9 context-switches          #    0.000 M/sec                  
                 0 CPU-migrations            #    0.000 M/sec                  
            16,695 page-faults               #    0.262 M/sec                  
       105,032,319 cycles                    #    1.649 GHz                     [81.35%]
        72,514,792 stalled-cycles-frontend   #   69.04% frontend cycles idle    [81.19%]
        52,701,496 stalled-cycles-backend    #   50.18% backend  cycles idle    [67.46%]
        51,685,651 instructions              #    0.49  insns per cycle        
                                             #    1.40  stalled cycles per insn [86.93%]
         9,379,517 branches                  #  147.291 M/sec                   [87.48%]
            32,447 branch-misses             #    0.35% of all branches         [85.32%]

       0.064535999 seconds time elapsed
看出task-clock和page-fault相差很大，

2、记录自己程序对应事件的数据：
sudo perf record -a -g -e task-clock  -c 1 ./ls

3、查看记录程序
sudo perf report
Events: 24K task-clock                                                                           
+  34.28%          swapper  [kernel.kallsyms]                   [k] intel_idle                  ◆
+   6.11%          swapper  [kernel.kallsyms]                   [k] poll_idle                   ▒
-   5.92%               ls  ls                                  [.] init_ob                     ▒
     init_ob                                                                                    ▒
     main                                                                                       ▒
     __libc_start_main                                                                          ▒
-   4.45%               ls  [kernel.kallsyms]                   [k] native_flush_tlb_single     ▒
   - native_flush_tlb_single                                                                    ▒
      - 72.44% __kunmap_atomic                                                                  ▒
         - 61.45% prep_new_page                                                                 ▒
              get_page_from_freelist                                                            ▒
            - __alloc_pages_nodemask                                                            ▒
               - 99.59% do_anonymous_page                                                       ▒
                    handle_pte_fault                                                            ▒
                    handle_mm_fault                                                             ▒
                    do_page_fault                                                               ▒
                  - error_code                                                                  ▒
                     - 99.39% init_ob                                                           ▒
                          main                                                                  ▒
                          __libc_start_main    

明显可以看出init_ob函数占用了很多。
编译-ggdb，使用annotate可以直接看出来。

